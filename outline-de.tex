\documentclass[ngerman,fleqn]{scrartcl}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{babel}

\usepackage[nobackrefs]{preamble}
\usepackage[subtle]{savetrees}

\pagestyle{empty}

\begin{document}

\medskip
\begin{center}
  \normalsize Kurzfassung der Arbeit\\
  \LARGE\textbf{Lernen Terminologischen Wissen\\ mit hoher Konfidenz aus fehlerhaften Daten}\\
  \bigskip%
  \large Dipl.-Math.\ Daniel Borchmann
\end{center}
\bigskip
\bigskip

\noindent
Wissen ist eine notwendige Voraussetzung für intelligentes Verhalten, und insbesondere für
Computer ist es daher unausweichlich zuerst relevantes Wissen zu erwerben um in gewissen
Situation entsprechend handeln zu können.  Das Problem des Erwerbs dieses Wissen,
allgemein als \emph{Knowledge Acquisition} oder auch \emph{Learning} bezeichnet, ist
gerade für Computer schwierig, da diese nicht, im Gegensatz zu Menschen, selbständig
lernen können.  Darüber hinaus besteht das Problem der Darstellung dieses Wissens
(\emph{Knowledge Representation}) in einer Form, die für eine maschinelle Verarbeitung
geeignet ist.  Um also intelligentes Verhalten von Maschinen zu erlauben muss sich zu erst
klar gemacht werden, wie eine solche Darstellung erreicht werden kann.

Ein populärer Ansatz für Knowledge Representation ist die Verwendung von Logik basierten
Formalismen, da diese ein vorhersagbares und verlässliches Verhalten versprechen, und
unter diesen Formalismen sind \emph{Beschreibungslogiken}~\cite{DLhandbook} sehr populär.
Durch die Verwendung beschreibungslogischer \emph{Wissensbasen} ist es möglich,
\emph{faktisch Wissen} und \emph{terminologisches Wissen} darzustellen.  Faktisches Wissen
gibt wissen über ein oder zwei \emph{Individuen} wieder; Beispiele dafür sind
\enquote{Abraham Lincoln war Präsident} oder \enquote{John McCarthy war Professor in
  Stanford}.  Terminologisches Wissen stellt allgemeines Wissen über alle Individuen dar,
die von Interessen sind.  Beispiele hierfür sind Aussagen wie \enquote{alle Menschen sind
  sterblich} oder \enquote{jeder Mensch hat einen Kopf}.  Diese Form von Wissen stellt
also Beziehungen zwischen verschiedenen \emph{Begriffen} dar, die in dem Gebiet der
Beschreibungslogiken mittels \emph{Konzeptbeschreibungen} repräsentiert werden.

Bei der Verwendung von beschreibungslogischen Wissensbasen bleibt das praktische Problem,
alles faktische und terminologische Wissen darzustellen, welches für die aktuelle
Anwendung relevant ist.  Dabei ist das Lernen faktischen Wissens relativ einfach, da man
hier nur für alle Individuen des Anwendungsbereichs die bekannten Fakten aufnehmen muss.
Das problematische hier ist meist nur die Art und Weise der Automatisierung, insbesondere
wenn Fakten aus unstrukturierten Datensätzen extrahiert werden sollen, wie zum Beispiel
Text.  Terminologisches Wissen kann allerdings auf diese Art und Weise nicht erhalten
werden, da es sich hier ja um Wissen handelt, welches auf \emph{alle} relevanten
Individuen gültig ist, und nicht nur für einige.  Darüber hinaus ist auch gar nicht klar,
wie man die Konzeptbeschreibungen erhalten soll, die in solchem Wissen in Beziehung
gesetzt werden.

Ein aktueller Ansatz für diese Problem wurde von Baader und Distel
erarbeitet~\cite{Diss-Felix}, welcher Ideen aus dem Gebiet der Formalen
Begriffsanalyse~\cite{fca-book} nutzt.  Die Grundidee dieser Arbeit ist es, dass die
Domäne, für die terminologisches Wissen erlangt werden soll, darstellbar ist als eine
\emph{endliche Interpretation}.  Interpretation sind relationale Strukturen, die verwendet
werden um die Semantik von Beschreibungslogiken zu definieren.  Intuitiv können diese
Strukturen gesehen werden als knoten- und kanten-beschriftet Graphen.  Baader und Distel
schlagen in diesem Fall vor, terminologisches Wissen für die so dargestellten Domäne zu
erhalten, indem \emph{endliche Basen} aller \emph{Allgemeiner Konzeptinklusionen}
berechnet werden, die in dieser Interpretation gültig sind und sich in der
Beschreibungslogik \ELbot ausdrücken lassen.  Allgemeine Konzeptinklusionen (\emph{general
  concept inclusions}, GCIs) bieten im Gebiet der Beschreibungslogiken einen allgemeinen
Ansatz, terminologisches Wissen darzustellen.  So lassen sich zum Beispiel unter
Verwendung von GCIs die obigen Beispiele darstellen als
\begin{equation*}
  \mathsf{Mensch} \sqsubseteq \mathsf{Sterblich}, \;\;\; \mathsf{Mensch} \sqsubseteq \exists
  \mathsf{hat}. \mathsf{Kopf}.
\end{equation*}
Diese zwei Konzeptinklusionen sind genau dann gültig in einer gewählten Interpretation,
wenn jeder Knoten (also jedes \emph{Individuum}) in dieser Interpretation, welches mit
\enquote{\textsf{Mensch}} beschriftet ist, auch mit \enquote{\textsf{Sterblich}}
beschriftet ist, und eine mit \enquote{\textsf{hat}} beschriftete, ausgehende Kante hat,
die auf ein Individuum zeit, welches \enquote{\textsf{Kopf}} als Beschriftung hat.
Endliche Basen $\mathcal{B}$ sind dann endliche Mengen gültiger GCIs, die
\emph{vollständig} sind in dem Sinne, dass alle gültigen GCIs bereits aus $\mathcal{B}$
\emph{folgen}.

Der Ansatz von Baader und Distel ist interessant für praktische Anwendungen, denn endliche
Interpretationen können auf Grund ihrer graphenartigen Struktur als eine andere
Darstellungsform von \emph{Linked Data}, das bevorzugte Datenformat des \emph{Semantic
  Web}, aufgefasst werden.  Die vorliegende Arbeit beginnt damit, diese
Anwendungsmöglichkeit an Hand eines Experimentes zu evaluieren, indem die Algorithmen von
Baader und Distel auf eine Teilmenge des Datensatzes des DBpedia-Projektes~\cite{DBpedia}
angewendet werden.  Es stellt sich dabei heraus, dass dieser Ansatz sich tatsächlich dazu
eignet, terminologisches Wissen aus Linked Data zu extrahieren.

% These experiments also reveal a disadvantage of the algorithm: considering only valid
% GCIs leads to a high sensitivity of the extracted knowledge to \emph{errors} in the
% data.  In other words, as soon as a single error is contained in the data, all valid
% GCIs affected by this error are not extracted anymore.  Since one cannot assume that
% data, and linked data in particular, is free of errors, this sensitivity severely
% impairs the usefulness of Baader and Distel's approach.
%
% One can assume, however, that the given data represents the domain of interest
% \enquote{sufficiently well}, in the sense that it does not contain too many errors.
% Based on this assumption, one can think of alleviating the sensitivity of Baader and
% Distel's approach to errors in the data by considering GCIs that are \enquote{almost
% valid} instead.  This way, GCIs that are erroneously invalidated by sporadic errors are
% still retrieved from the data.
%
% It is the main goal of this thesis to generalize the results by Baader and Distel
% accordingly.  For this, the notion of \emph{confidence} as used in
% data-mining~\cite{arules:agrawal:association-rules} is transferred to GCIs.
% Intuitively, the confidence of a GCI $C \sqsubseteq D$ in a finite interpretation
% $\mathcal{I}$ quantifies how often this GCI is correct, in the sense that if an
% individual in $\mathcal{I}$ satisfies the concept description $C$, then it also
% satisfies the concept description $D$.  For example, the confidence of $\mathsf{Human}
% \sqsubseteq \mathsf{Mortal}$ in a finite interpretation would be the number of
% individuals labeled with both $\mathsf{Human}$ and $\mathsf{Mortal}$, divided by the
% number of individuals just labeled with $\mathsf{Human}$.
%
% The notion of confidence of GCIs gives a measure of \enquote{how valid} a GCI is in a
% finite interpretation $\mathcal{I}$, and it can be used to formalize the notion of being
% \enquote{almost valid}: a GCI is \emph{almost valid} in $\mathcal{I}$ if and only if its
% confidence in $\mathcal{I}$ is above a chosen \emph{confidence threshold} $c \in [0,1]$.
% Such an almost valid GCI is also called a GCI \emph{with high confidence}.  The task is
% then to find finite bases for such GCIs with high confidence, \ie to find finite sets
% $\mathcal{B}$ of GCIs with high confidence that are \emph{complete}, in the sense that
% all GCIs with high confidence are entailed by $\mathcal{B}$.
%
% First results on computing such bases are achieved by exploiting the close connection
% between description logics and formal concept analysis that has already been used in the
% approach by Baader and Distel.  Based on this connection as well as on results by
% Luxenburger on \emph{partial implications} in formal contexts~\cite{diss:Luxenburger},
% this thesis discusses methods to explicitly describe bases of GCIs with high confidence.
% Furthermore, results are obtained that allow to compute such bases from bases of
% implications with \emph{high confidence} in formal contexts.  All these findings are
% applied to the data set used for the experiments above, and are evaluated for their
% practicability with respect to handling sporadic errors.
%
% A drawback of this approach is that the consideration of GCIs with high confidence is
% purely heuristic, in the sense that counterexamples to GCIs are ignored if they are just
% not frequent enough.  In particular, if the initial interpretation contains rare but
% valid counterexamples of a given GCI, then these are ignored as well.  To remedy this,
% an external source of information that is able to distinguish between these rare
% counterexamples and errors in the data is necessary.  An example for such a source would
% be a human expert for the domain of interest.
%
% Given such an expert and a finite base of GCIs with high confidence, one could address
% this problem by letting an expert verify every GCI contained in this base.  All GCIs
% rejected by the expert are then removed.  However, this naive approach has the
% disadvantage that as soon as a GCI is removed, other GCIs may have to be added to ensure
% completeness of the base, and it is by far not clear how this can be done without
% recomputing the base completely.
%
% In this thesis, the more systematic approach of \emph{model exploration by confidence}
% is introduced.  This is an interactive learning algorithm based on \emph{model
% exploration}, an algorithm devised by Baader and Distel to address the problem of
% \emph{incomplete data}, and is itself based on \emph{attribute exploration}, an
% interactive knowledge acquisition algorithm from formal concept analysis.
%
% Model exploration by confidence is essentially an interactive algorithm to compute
% finite bases of GCIs with high confidence.  During the run of this algorithm, every
% computed GCI is given to the expert for verification.  If the expert confirms this GCI,
% it is added to the base.  If it is rejected, the expert is required to provide
% \emph{valid} counterexamples for this GCI, which are then added to the data.  When the
% algorithm terminates, the set of confirmed GCIs constitutes a base.  Moreover,
% counterexamples provided by the expert are not subject to the confidence heuristics, \ie
% as soon as a counterexample invalidates a GCI, it is not considered any further, even if
% its confidence is high enough.  In this way, rare counterexamples can be distinguished
% from errors.
%

\printbibliography{}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% ispell-local-dictionary: "de_DE"
%%% End: 

%  LocalWords:  Konzeptinklusionen graphenartigen
