\documentclass[english,fleqn]{scrartcl}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{babel}

\include{preamble}

\pagestyle{empty}

\begin{document}

\medskip
\begin{center}
  \normalsize Abstract of the thesis\\
  \LARGE\textbf{Learning Terminological Knowledge\\ with High Confidence from Erroneous
    Data}\\
  \bigskip%
  \large Dipl.-Math.\ Daniel Borchmann
\end{center}
\bigskip
\bigskip

\noindent
Knowledge is a necessary requirement for intelligent behavior.  For computers, in
particular, it is therefore indispensable to first obtain relevant knowledge before they
can act intelligently in certain situations.  The problem of obtaining this knowledge,
commonly referred to as the problem of \emph{knowledge acquisition} or just
\emph{learning}, is hard to approach, because computers, in contrast to human beings, are
not able to learn autonomously.  Moreover, for computers it is relevant that the knowledge
they obtain is represented in a way that is suitable for them to work with.  Thus, the
problem of \emph{knowledge representation} has also to be tackled to enable intelligent
behavior of machines.

A popular choice to represent knowledge for computers is to use logic-based formalisms,
because they yield predictable and reliable behavior.  In the area of logic-based
knowledge representation formalism, \emph{description logics} are one of the most
successful approaches.  Using \emph{description logic knowledge bases}, it is possible to
represent \emph{assertional} and \emph{terminological} knowledge.  Assertional knowledge
express facts about one or two individuals the domain for which knowledge has to be
learned.  Examples for this kind of knowledge are \enquote{Abraham Lincoln was a
  president}, or \enquote{John McCarthy was a professor at Stanford University}.  In other
words, assertional knowledge expresses \emph{assertions} about certain individuals.
Terminological knowledge represents common knowledge about all individuals of the domain
of interest.  Examples for this kind of knowledge are \enquote{all humans are mortal} or
\enquote{every human has a head}.  Thus, terminological knowledge represents dependencies
between different \emph{terms}, called \emph{concept descriptions}.

If one chooses to represent knowledge using description logics, one is left with the
problem to obtain all relevant assertional and terminological knowledge about the domain
one is interested in.  Obtaining assertional knowledge is relatively easy to obtain:
assuming that one knows all individuals of the domain, one can gather all facts that are
known about these.  The difficulty here lies mostly in the problem automating this
process, in particular if one tries to gather facts from unstructured data, like text,
that is hard to process for computers.  On the other hand, obtaining terminological
knowledge cannot be achieved this way, since one is looking for general knowledge that
affects \emph{all} individuals of the domain.  Moreover, it is not at all easy to find the
concept descriptions which appear in this kind of knowledge.

Recent work by Baader and Distel~\cite{Diss-Felix} proposes an approach to learning
terminological knowledge by computing valid \emph{general concept inclusions} (GCIs) of
\emph{finite interpretations} which are expressible in the description logic \ELbot.  GCIs
are the most general means to represent terminological knowledge in description logics.
For example, the examples from above can be represented by GCIs as
\begin{equation*}
  \mathsf{Human} \sqsubseteq \mathsf{Mortal}, \; \mathsf{Human} \sqsubseteq \exists
  \mathsf{has}. \mathsf{Head}.
\end{equation*}
Interpretations, on the other hand, are used to define the semantics of description
logics, and they can best be thought of as directed edge- and vertex-labeled graphs.  The
above two GCIs are valid in an interpretation if every node that is labeled with
\textsf{Human} is also labeled with \textsf{Mortal}, and has an outgoing edge labeled with
\textsf{has} that points to a node labeled with \textsf{Head}.

The approach of Baader and Distel is interesting for practical applications.  This is
because finite interpretations can be seen as a different form of \emph{linked data}, the
data-format used by the \emph{Semantic Web}.  The first contribution of this work is to
provide an implementation of the algorithms by Baader and Distel, and to apply them to
linked data obtained by the DBpedia project~\cite{DBpedia}.  The results of these
experiments show that this approach indeed allows us to learn terminological knowledge
from data.

These experiments, however, also reveal a disadvantage of the algorithm: considering only
valid GCIs leads to a high sensitivity of the extracted knowledge with respect to
\emph{errors} in the data.  In other words, as soon as one error is present in the data,
all valid GCIs affected by this error are not extracted anymore.  Since one cannot assume
that data, and linked data in particular, is free of errors, this sensitivity severely
impairs the usefulness of Baader and Distel's approach.



\printbibliography{}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
