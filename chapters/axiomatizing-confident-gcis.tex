\chapter{Axiomatizing Confident \EL-GCIs of Finite Interpretations}
\label{cha:axiom-conf-el}

The results obtained by Distel about computing finite bases of finite interpretations are
not only interesting from a theoretical point of view.  Although we have skipped most of
the details, all the relevant results are \emph{effective} in the sense that the obtained
bases can in principle be computed by computers.  Thus, these results may also be
interesting for practical applications.

A possible application of Distel's results is to compute bases from \emph{Linked Open
  Data}~\cite{Linked-Data}, a format for representing data as used by the semantic
web~\cite{journal/sciam/BernersLeeHL01,DBLP:conf/dagstuhl/2003sweb}\todo{add reference to
  \emph{Foundations of Semantic Web Technologies}}.  This data format is mainly
constituted of RDF triples and can be thought of as an edge-labeled graph.  As such, it is
very similar to interpretations, and thus Distel's results are applicable here.

As a first contribution of this thesis we have implemented the major results obtained by
Distel on computing finite bases as described previously, and applied them to a particular
data set of the \emph{Linked Open Data Cloud}, namely to a subset of the DBpedia data
set~\cite{DBpedia}.  In \Cref{sec:computing-bases-from} we describe this experiment in
detail and show what Distel's results yield when applied to this data set.  This
experiment has also been discussed previously
in~\cite{Borchmann:confident-GCIs,DBLP:conf/icdm/BorchmannD11}.

One conclusion from this experiment is that Distel's results are very sensitive to
\emph{errors} in the data.  This is actually not surprising: bases of finite
interpretations only contain general concept inclusion which are valid in the data, and if
there is only one single counterexample to a general concept inclusions, it will not be
contained in any base.

If those counterexamples are erroneous, however, then this can cause problems.  Not only
that otherwise valid general concept inclusions are not obtained by Distel's approach
anymore.  Sporadic erroneous counterexamples may also cause GCIs found during the
computation of bases to be rather complicated, because those GCIs have to avoid those
erroneous counterexamples.

To remedy, or at least to alleviate this effect of erroneous counterexample to computing
finite bases we shall consider an extension of Distel's results which tries to find bases
of GCIs which are not necessarily valid in the given interpretation, but instead enjoy a
\emph{high confidence} in it.  The notion of \emph{confidence} is borrowed from
data-mining~\cite{arules:agrawal:association-rules}, more precisely from the theory of
\emph{association rules}, and allows to measure how much an association rule is allowed to
ignore counterexamples.  We shall transfer the notion of confidence to general concept
inclusions, and shall then try to find bases for those \emph{GCIs with high confidence}.

For this we shall make use of results obtained by Luxenburger from his work on
\emph{partial implications}~\cite{diss:Luxenburger,Luxenburger91}, and extensions
thereof~\cite{DBLP:conf/ki/StummeTBPL01}.  Partial implications can be thought of as
implications considered together with their confidence in some particular formal context.
We shall discuss in \Cref{Luxen-base} how these results allow us to obtain bases of all
implications which have \emph{high confidence} in some given formal context.

In \Cref{sec:first-base} we then show how these ideas can be simulated in \ELgfpbot to
find bases for GCIs with high confidence.  Moreover, we shall also show in
\Cref{sec:bases-confident-gcis} how bases of implications with high confidence yield bases
of GCIs with high confidence.  Finally, we shall also discuss a minimality result in
\Cref{sec:minimality-result} which is similar to the one given in \Cref{thm:Felix-5.18},
as well as how to obtain \ELbot bases of \ELgfpbot bases for GCIs with high confidence.

The results thus obtained are again all effective, and we shall discuss some experiments
in \Cref{sec:exper-with-conf} which use the same data-set as the one used in
\Cref{sec:computing-bases-from}.  This allows us to directly compare the approaches of
computing bases of valid GCIs on the one hand, and bases of GCIs with high confidence on
the other.  Moreover, we shall also discuss shortcomings of the approach of considering
GCIs with high confidence, which will eventually lead us to considering extensions of the
attribute exploration algorithm.  These will be discussed in \Cref{cha:expl-conf} and
\Cref{cha:model-expl-conf}.

\section{Computing Bases from DBpedia}
\label{sec:computing-bases-from}

We want to evaluate the practicability of Distel's results by applying them to linked data
extracted from the Linked Open Data Cloud.  In other words, given some linked data, we
want to extract a complete set of general concept inclusions that is valid within this
data set.  The goal of this experiment is to see in how far Distel's approach is practical
in learning terminological knowledge about some domain which is represented by linked data.

Of course, before we can do so we first have to discuss how we can obtain an
interpretation from a given linked data set, and one which sufficiently reflects the
logical structure of the initial data set.

Recall that a finite interpretation $\mathcal{I} = (\Delta^{\mathcal{I}},
\cdot^{\mathcal{I}})$ over $N_C$ and $N_R$ consists of a set $\Delta^{\mathcal{I}}$ and a
mapping $\cdot^{\mathcal{I}}$ that maps every $A \in N_C$ to a set $A^{\mathcal{I}}
\subseteq \Delta^{\mathcal{I}}$, and every $r \in N_R$ to a set of pairs $r^{\mathcal{I}}
\subseteq \Delta^{\mathcal{I}} \times \Delta^{\mathcal{I}}$.  We have also already seen
some examples of depicting interpretations as \emph{graphs}, more precisely as
\emph{directed edge- and vertex-labeled graphs}.  Indeed, interpretations are essentially
nothing else than those graphs, where the set of vertex labels is $N_C$ and the set of
edge labels is $N_R$.

Linked data is now quite similar to labeled graphs.  More precisely, linked data is just
an edge-labeled graph, represented by so-called \emph{RDF-Triples}.  Every triple consists
of a \emph{subject}, a \emph{predicate}, and an \emph{object} (in that order), each of
them being an \emph{uniform resource identifier} (URI).  The idea is that RDF-Triples
encode the information that the subject is connected to the object by means of the
predicate.  Two examples of RDF-Triples, taken from the DBpedia data set~\cite{DBpedia},
are\footnote{Indeed, these are \emph{serializations} of RDF-Triples, in this case in the
  so-called \emph{N-Triples} format}
\begin{verbatim}
  <http://dbpedia.org/resource/Aristotle>
  <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>
  <http://dbpedia.org/ontology/Philosopher> .

  <http://dbpedia.org/resource/Aristotle>
  <http://dbpedia.org/ontology/influenced>
  <http://dbpedia.org/resource/Western_philosophy> .
\end{verbatim}
Intuitively, these triples encode the facts that \emph{Aristotle is (was) a philosopher}
and that \emph{Aristotle influenced Western Philosophy}.

Since RDF-Triples constitute edge-labeled graphs, we can take them as they are and regard
them as interpretations over $N_C = \emptyset$ and $N_R$, where $N_R$ is just the set of
all predicates which appear in RDF-Triples in the data set.  This approach would work,
however it would only yield GCIs where no concept names are present.  Such terminological
knowledge may not be very interesting.

To alleviate this problem we make use of the special RDF predicate
\begin{equation}
  \label{eq:16}
  \verb|http://www.w3.org/1999/02/22-rdf-syntax-ns#type|
\end{equation}
which expresses that a subject is an instance of a certain
class.\footnote{\url{http://www.w3.org/1999/02/22-rdf-syntax-ns}} If we consider triples
with this predicates not as edges in the linked graph, but instead as the information that
the subject is an instance of the object, then we indeed consider linked data as a vertex-
and edge-labeled graph with vertex labels $N_C$ and edge-labels $N_R$, where $N_C$ is not
empty in general.

This view on linked data now allows us to consider it as an interpretation over some sets
$N_C$ and $N_R$, and to apply Distel's result to such data sets.  For our experiments, we
have chosen a subset of the DBpedia data set as of March 2010 (Version
3.5)\footnote{\url{http://wiki.dbpedia.org/Downloads35?v=pb8}}.  The linked data contained
within the DBpedia data-set has been extracted automatically from \emph{Wikipedia
  Infoboxes}, which are a means to represent facts about the topic of the current page in
a compact way.\todo{picture?}

The subset of the DBpedia data-set we used for our experiments arises by restricting our
attention to the relation
\begin{equation*}
  \verb|http://dbpedia.org/ontology/child|.
\end{equation*}
From this subset, we constructed an interpretation $\Idbpedia = (\Delta^{\Idbpedia},
\cdot^{\Idbpedia})$.  To make the following considerations easier to read we shall drop
the prefix and just write \textsf{child} instead of
\texttt{http://dbpedia.org/ontology/child}, for example.

To construct $\Idbpedia$ we first computed all triples $T_{\mathsf{child}}$ from the
DBpedia data set whose predicate is \textsf{child}.  The subjects of these triples where
collected into $\Delta^{\Idbpedia}$.  We defined
\begin{equation*}
  \mathsf{child}^{\Idbpedia} := \set{ (s, o) \in \Delta^\Idbpedia \times \Delta^\Idbpedia
    \mid (s, \mathsf{child}, o) \in T_{\mathsf{child}} }.
\end{equation*}
Examples for triples contained in $T_{\mathsf{child}}$ are
\begin{verbatim}
  <Abraham_Lincoln> <child> <Robert_Todd_Lincoln> .
  <Abraham_Lincoln> <child> <Edward_Baker_Lincoln> .
\end{verbatim}
Therefore,
\begin{align*}
  (\mathsf{Abraham\_Lincoln}, \mathsf{Robert\_Todd\_Lincoln}) &\in \mathsf{child}^{\Idbpedia},\\
  (\mathsf{Abraham\_Lincoln}, \mathsf{Edward\_Baker\_Lincoln}) &\in \mathsf{child}^{\Idbpedia}.
\end{align*}

Then we considered all triples $T_{\mathsf{type}}$ whose predicate is the special type
predicate of \Cref{eq:16} and whose subject is contained in $\Delta^{\Idbpedia}$.  The
objects of those triples where collected into a set $N_C$, \ie they constituted the
concept names of $\Idbpedia$.\footnote{We omitted
  \texttt{http://www.w3.org/2002/07/owl\#Thing} in $N_C$, as it does not introduce any
  meaningful information} Then, for an $A \in N_C$, we defined $A^{\Idbpedia}$ to be the
set of all subjects which appear in a triple in $T_{\mathsf{type}}$, \ie
\begin{equation*}
  A^{\Idbpedia} := \set{ s \in \Delta^{\Idbpedia} \mid (s, t, A) \in
    T_{\mathsf{type}} }
\end{equation*}
where $t$ stands for the special RDF type predicate of \Cref{eq:16}.  Example triples from
$T_{\mathsf{type}}$ concerning \verb|Abraham_Lincoln| are
\begin{verbatim}
  <Abraham_Lincoln>
  <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>
  <Person> .

  <Abraham_Lincoln>
  <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>
  <OfficeHolder> .
\end{verbatim}
and therefore
\begin{align*}
  \mathsf{Abraham\_Lincoln} &\in \mathsf{Person}^{\Idbpedia}, \\
  \mathsf{Abraham\_Lincoln} &\in \mathsf{OfficeHolder}^{\Idbpedia}.
\end{align*}

The interpretation $\Idbpedia$ then 5624 elements and contained 60 concept names, \ie
$\abs{ N_C } = 60$.  By construction, that there is only one role name in $\Idbpedia$,
namely \textsf{child}.  To now compute bases of $\Idbpedia$ means to extract all knowledge
about the child-relation present in DBpedia and expressible in \ELbot.  Note that elements
from DBpedia are only present in $\Idbpedia$ if they have children, or are children of
someone else.  Moreover, as DBpedia extracts its information from Wikipedia, all elements
in $\Idbpedia$ correspond to articles in Wikipedia; in particular, if such elements
correspond to persons, then those persons have to be ``sufficiently famous'' in the sense
that they deserve a Wikipedia article.  Therefore, $\Idbpedia$ represent DBpedia's
knowledge about famous persons and their famous children.

We have to note, however, that the \emph{child}-relation in DBpedia contains some false
information, mostly due to the way this information is extracted from Wikipedia Infoboxes.
More precisely, our interpretation $\Idbpedia$ not only contains elements which correspond
to humans, but also contains elements which are instances of \textsf{Work},
\textsf{Organisation} or \textsf{PopulatedPlace}, among others.  However, those artifacts
are comparably rare, and it is still reasonable to use $\Idbpedia$ for our experiments.

We have implemented the algorithms devised by Distel to compute bases of finite
interpretations\footnote{\url{http://github.com/exot/EL-exploration}} on top of
\texttt{conexp-clj}\footnote{\url{http://github.com/exot/conexp-clj}}, a general purpose
tool for formal concept analysis.  When computing a minimal base of $\Idbpedia$ as
described in \Cref{thm:Felix-5.18}, we obtained a base $\mathcal{B}_{\Idbpedia}$
containing 1252 general concept inclusions.  In the following we want to examine the GCIs
contained in this bases, describe some observations we made, and discuss the usefulness of
these GCIs.  Of course, we cannot do this formally, and shall therefore only argue
intuitively.

Firstly, some of the GCIs contained in $\mathcal{B}_\Idbpedia$ constitute knowledge about
the relationships among the concept names occurring in $\Idbpedia$ only, for example
\begin{align*}
  \sf Politician &\sqsubseteq \sf Person \\
  \sf MemberOfParliament &\sqsubseteq \sf Person \sqcap Politician \\
  \sf Criminal \sqcap Politician &\sqsubseteq \bot
\end{align*}
This knowledge can indeed be useful to learn the hierarchy of concept names
(\emph{taxonomy}) of $\Idbpedia$.  On the other side, this knowledge does not yet show
whether Distel's results are useful, as this knowledge could have easily been obtained my
methods from formal concept analysis alone.

The last GCI states that there are no elements in $\Idbpedia$ that are both criminal and
politicians.  GCIs of this form are called \emph{disjointness constraints}, and they can
be useful in applications.\todo{cite paper by Felix and Yue?}  The disjointness constraint
shown above only contains concept names.  However, Distel's approach let's us find more
disjointness constraints than just between concept names.  For example,
$\mathcal{B}_{\Idbpedia}$ contains the GCI
\begin{equation*}
   \sf Philosopher \sqcap \exists child. \top \sqsubseteq \bot
\end{equation*}
expressing that philosophers don't have children (or at least not children famous enough
to occur in Wikipedia).  Indeed, since $\mathcal{B}_{\Idbpedia}$ is complete for
$\Idbpedia$, all disjointness constraints valid in $\Idbpedia$ can either be found in this
base or are entailed by it.  Such information can be of practical relevance.

There are other GCIs contained in $\mathcal{B}_\Idbpedia$ which are not disjointness
constraints or only express knowledge about concept names.  Two of them for which we could
argue that they can be useful are
\begin{align*}
  \sf \exists child. Person &\sqsubseteq \sf Person, \\
  \sf FictionalCharacter \sqcap \exists child. Person &\sqsubseteq \sf \exists child. FictionalCharacter
\end{align*}
where the second GCI can be seen as some kind of \ELbot approximation of the fact that
fictional characters can only have fictional characters as their children.  These GCIs can
indeed be seen as useful terminological knowledge for the domain represented by
$\Idbpedia$.

Those GCIs, \ie where one could argue that they represent meaningful knowledge, are quite
rare in $\mathcal{B}_\Idbpedia$.  On the other hand, $\mathcal{B}_\Idbpedia$ contains many
GCIs whose usefulness is highly doubtful, either because they combine otherwise unrelated
concepts, or they are too specific.

An example for the first case is
\begin{equation*}
  \sf Person \sqcap \exists child. Book \sqsubseteq FictionalCharacter
\end{equation*}
which does not really represent any meaningful knowledge.  On the other hand, this GCI
indicates that DBpedia finds books as children only on Wikipedia pages on fictional
characters.  Thus, such GCIs may help to find errors in the way DBpedia extracts
information, but are not helpful as knowledge itself.  Similar examples are
\begin{align*}
  \sf \exists child. Newspaper \sqcap Person &\sqsubseteq \sf Writer, \\
  \sf \exists child. MilitaryUnit &\sqsubseteq \sf Judge, \\
  \sf \exists child. Settlement \sqcap Politician &\sqsubseteq \sf Congressman.
\end{align*}

GCIs which could be too specific can be considered to most common case among all GCIs in
$\mathcal{B}_{\Idbpedia}$, examples of them being
\begin{gather*}
  \sf \exists child. Ambassador \sqcap OfficeHolder \\
  \sqsubseteq \sf \exists child. (Ambassador \sqcap \exists child. Person) \sqcap {}\\
  \phantom{\sqsubseteq{}} \sf\exists child. (OfficeHolder \sqcap \exists child.
  Congressman \sqcap \exists child. Actor \sqcap \exists child OfficeHolder).
\end{gather*}
Indeed, this GCI only applies to the individual \texttt{Joseph\_P.\_Kennedy\%2C\_Sr.}, and
thus it states information only about this very individual and its children.  To cope with
such situations, Distel extended his results about computing bases of finite
interpretations to \emph{model exploration} which also allows for \emph{expert
  interaction}, in a way similar to attribute exploration in formal concept analysis.  If
the expert would encounter such a GCI she would reject it as being too specific, and would
provide more examples to make it more general.  We shall discuss this algorithm in more
detail in \Cref{cha:model-expl-conf}.

Another approach to eliminating those over-specific GCIs may be to require for the
extracted GCIs that they apply to at least a certain number of different individuals.
However, this approach leads to some unintuitive behavior, see \Cref{cha:conclusions}.

Finally, among the GCIs contained in $\mathcal{B}_\Idbpedia$ there exist some which convey
the impression of being redundant, like
\begin{equation*}
  \sf \exists child. \exists child. \top \sqsubseteq \exists child. (Person \sqcap \exists child. \top),
\end{equation*}
because it should be quite clear from the construction of $\Idbpedia$ that only persons
can have children, \ie the following GCI
\begin{equation}
  \label{eq:23}
  \sf \exists child. \top \sqsubseteq Person
\end{equation}
should hold in $\Idbpedia$.  This is because only Wikipedia articles of human beings
should contain a child-entry within their Infobox.  Thus, even with all the errors in
$\Idbpedia$ concerning non-persons that we have discussed before, the GCI in \Cref{eq:23}
should hold in $\Idbpedia$.

However, this is not the case, since there are four counterexamples to this GCI in
$\Idbpedia$, \ie elements $x \in \Delta^{\Idbpedia}$ satisfying $x \in (\sf \exists
child. \top)^{\Idbpedia} \setminus Person^{\Idbpedia}$.  These
are\footnote{Coincidentally, all these are artists from Hong Kong}
\begin{equation*}
  \mathtt{Teresa\_Carpio},\; \mathtt{Charles\_Heung},\;
  \mathtt{Adam\_Cheng}, \; \mathtt{Lydia\_Shum}.
\end{equation*}
However, these elements clearly represent human beings, so they \emph{should} actually be
instances of \textsf{Person}.  In other words, these counterexamples are all
\emph{erroneous} counterexamples, but since they are present in $\Idbpedia$ the approach
developed by Distel will not ignore them.  This not only inhibits finding the GCI $\sf
\exists child. \top \sqsubseteq Person$, but also causes incomprehensible GCIs to be
found which are special cases of this GCI but somehow try to ``circumvent'' the erroneous
counterexamples. An example for this is
\begin{gather*}
  \sf Person \sqcap \exists child. (Person \sqcap \exists child. (Person \sqcap \exists child.
  (Person \sqcap {}\\
  \quad \quad \sf \exists child. \exists child. (Person \sqcap \exists child. \top))))
  \\
  \sf \quad \sqsubseteq
  \exists child. (Person \sqcap \exists child. (Person \sqcap \exists child. (Person \sqcap
  \exists child. (Person \sqcap {}\\
  \sf \quad \quad \exists child. \exists child. Person)))).
\end{gather*}

\section{GCIs with High Confidence in Finite Interpretations}
\label{sec:confident-gcis}

It would certainly increase the applicability of Distel's approach if we could ignore
erroneous counterexamples in our data, as then we could expect simpler and more general
GCIs to be extracted.  However, we cannot expect to have an automatic procedure which
achieves this goal, \ie we cannot expect an algorithm that automatically ignores erroneous
counterexamples, as for this the algorithm would need to know what errors are in the
current domain of interest, and for this the algorithm would need to obsess knowledge
about this domain, which however we are just about to learn.

On the other hand, what we can assume is that our initially given interpretation
$\mathcal{I}$ contains only \emph{few} errors, as otherwise learning GCIs from it would be
futile.  Based on this assumption we can approach the problem of erroneous counterexamples
as follows: if it is the case that $\mathcal{I}$ contains much more \emph{positive
  examples} for a GCI $C \sqsubseteq D$ than \emph{negative} ones, then we assume that the
negative counterexamples are ``probably erroneous.''  Here, a \emph{positive example} for
$C \sqsubseteq D$ would be an element $x \in \Delta^{\mathcal{I}}$ satisfying $x \in
C^{\mathcal{I}} \cap D^{\mathcal{I}}$, and a \emph{negative example} for $C \sqsubseteq D$
would be an element $y \in \Delta^{\mathcal{I}}$ such that $y \in C^{\mathcal{I}}
\setminus D^{\mathcal{I}}$.  We can consider such a GCI to be ``almost valid'' in
$\mathcal{I}$.  The approach would then be to consider these ``almost valid'' GCIs in
addition to the valid ones, and try to find finite bases for both of them.

This approach would actually work quite well for our example interpretation $\Idbpedia$,
as the GCI $\sf \exists child. \top \sqsubseteq Person$ has much more positive than
negative examples: there are 2551 elements $x \in \Delta^{\Idbpedia}$ to which $\sf
\exists child. \top \sqsubseteq Person$ applies, \ie which satisfy $x \in (\sf \exists
child. \top)$, but only 4 of those (the ones mentioned above) fail to also satisfy $x \in
\sf Person^{\Idbpedia}$.  Therefore, $\sf \exists child. \top \sqsubseteq Person$ has 2547
positive examples, but only 4 negative ones.  By our approach, we consider these negative
examples as errors and ignore them.  Thus, $\sf \exists child. \top \sqsubseteq Person$
would be extracted from $\Idbpedia$.

Of course, this approach is highly heuristic in general: if counterexamples for a GCI $C
\sqsubseteq D$ are just \emph{rare} in $\mathcal{I}$, then the above sketched approach
would treat them as errors, which is incorrect.  On the other hand, it may be much more
desirable to extract GCIs which are \emph{wrong} in some application domain, than to miss
GCIs which are \emph{correct}, because identifying wrong GCIs is much easier than finding
correct GCIs that are just invalidated by errors in the data.

It is the purpose of this section to give a formalization of the notion of a GCI to be
``almost valid'' in some finite interpretation.  We shall base this formalization on the
notion of \emph{confidence} as it is used in
data-mining~\cite{arules:agrawal:association-rules}.  This will be done in
\Cref{sec:conf-gcis-conf}.  Our goal is then to find \emph{finite bases} of all GCIs which
enjoy a \emph{high confidence} in the initially given interpretation.  For this we shall
review in \Cref{Luxen-base} ideas from Luxenburger from this research on \emph{partial
  implications} in formal contexts.  Based on this, we shall discuss the computation of
finite bases of GCIs with high confidence, first by mimicking Luxenburger's results by in
\ELgfpbot (~\Cref{sec:first-base}), and then by directly using methods from formal concept
analysis (~\Cref{sec:bases-confident-gcis}).  We shall also discuss a minimality result in
\Cref{sec:minimality-result} which is akin to the one obtained by Distel in
\Cref{thm:Felix-5.18}.  Finally, we shall discuss how \ELgfpbot bases for GCIs with high
confidence can be turned into \ELbot bases for GCIs with high confidence.

\subsection{Confidence of GCIs and Confident Bases}
\label{sec:conf-gcis-conf}

We want to formalize the notion of a GCI $C \sqsubseteq D$ to be ``almost valid'' in some
given interpretation $\mathcal{I}$.  For this we argued intuitively that the number of
positive examples should be ``much higher'' then the number of negative examples.  For
this, we define the \emph{confidence} of $C \sqsubseteq D$ in $\mathcal{I}$.

\begin{Definition}[Confidence of GCIs]
  \label{def:gci-confidence}
  
\end{Definition}

\todo[inline]{Write: introduce notion of confidence for GCIs of finite interpretations}%

\subsection{Luxenburger's Base}
\label{Luxen-base}

\todo[inline]{Write: confidence of implications}%
\todo[inline]{Write: introduce work of Luxenburger}%

\subsection{A Luxenburger-Style Base of all Confident GCIs}
\label{sec:first-base}

\todo[inline]{Write: adapt ideas from Luxenburger}%
\todo[inline]{Write: use neighborhood-relation}%
\todo[inline]{Write: show how to compute this base from the induced context}

\subsection{Bases of Confident GCIs from Bases of Confident Implications}
\label{sec:bases-confident-gcis}

\todo[inline]{Write: compute confident bases of GCIs from bases of confident implications
  (12-06/4.3) }%

\subsection{A Minimality Result}
\label{sec:minimality-result}

\todo[inline]{Write: completing sets and the minimality result}

\subsection{Unravelling \ELgfpbot Bases into \ELbot Bases}
\label{sec:unrav-elgfpb-bases}

\todo[inline]{Write: introduce unravelling of \ELgfpbot concept descriptions}%
\todo[inline]{Write: show how unravelling can be used to obtain \ELbot-Bases from
  \ELgfpbot ones (12-06/6)}%

\section{Experiments with Confident GCIs}
\label{sec:exper-with-conf}

\todo[inline]{Write: recall initial experiments, and show how the approach performs}%
\todo[inline]{Write: include results from 12-06/5}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../main"
%%% End: 
%  LocalWords:  unintuitive Organisation PopulatedPlace OfficeHolder rdf ns Infobox conf
%  LocalWords:  gcis DBpedia's
