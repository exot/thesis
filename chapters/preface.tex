\vspace*{0.38196601125\textheight}
\begin{center}
  \begin{minipage}[c]{0.5\linewidth}
    \selectlanguage{ngerman}
    \enquote{Wenn ich nur erst die Sätze habe! Die Beweise werde ich schon finden.}
    
    \bigskip
    
    \selectlanguage{english}
    \enquote{If only I had the theorems! Then I should find the proofs easily enough.}
    
    \medskip
    
    \hbox{}\hfill\tikz[baseline=-0.6ex,thick] \draw (0,0) -- (1,0); {\small Bernhard Riemann}    
  \end{minipage}
\end{center}

\newpage

\chapter*{Preface}
\label{cha:preface}

\thispagestyle{empty}

Knowledge is necessary to solve problems and to act intelligently in almost all situations
-- a simple fact that is true for humans and computers alike.  Therefore, to enable humans
and computers to act intelligently, it is necessary to first solve the problem of
\emph{learning relevant knowledge}, more formally referred to as \emph{knowledge
  acquisition}.  In the case of computers, another problem has to be tackled, namely the
problem of \emph{representing knowledge} in a way that is suitable for a machine
consumption.

A common approach to representing knowledge suitable for computer consumption is to use
different kinds of logics, and among these \emph{description logics} are a very popular
choice.  Description logics are a family of logic-based formalisms with varying reasoning
complexity and expressiveness, specifically tailored towards practical decision
procedures.  Moreover, \emph{description logic knowledge bases} (also called
\emph{ontologies}) allow for a powerful mechanism to represent knowledge, which is used in
practical applications like bio-medical ontologies and the semantic web.

The types of knowledge representable by description logic knowledge bases are
\emph{assertional knowledge} and \emph{terminological knowledge}.  Obtaining assertional
knowledge, like the fact that Abraham Lincoln was a president, or that John McCarthy was a
professor at Stanford University, is comparably easy, and can (to a certain extent) even
be done automatically.  This is mostly due to the rather local nature of assertional
knowledge, which just concerns one or two individuals.  On the other hand, obtaining
terminological knowledge is much more involved, because this knowledge relates concepts,
and not individuals, to each other: the fact that every human is mortal, or that every
human has a head are examples for terminological knowledge.  Obtaining such knowledge,
which may concern a wide and mainly indefinite range of individuals or things, is much
harder, and hoping to obtain it in an automatic way seems gullible.

Nevertheless, there have been various approaches to obtaining terminological knowledge, at
least in a preliminary version, from various sources of data, like natural language texts,
databases, or linked data.  One of these approaches has been developed by Franz Baader and
Felix Distel, and is based on notions from the mathematical theory of \emph{formal concept
  analysis}.  This approach allows the extraction of general terminological knowledge in
the form of \emph{general concept inclusions} that are valid in a given \emph{finite
  interpretation}.  Interpretations are structures, which are used in description logics
to define the semantics of different logics, and can be thought of as directed edge- and
vertex-labeled graphs.  Because of their graph-like nature, interpretations can be thought
of as a variant of \emph{linked data}, and then the approach by Baader and Distel in
principle allows the extraction of terminological knowledge from this form of data, which
is ubiquitous in the realm of the semantic web.  Thus, one could turn the vast amount of
linked data into description logic knowledge bases and use them to make computers act more
intelligently.

However, the approach of Baader and Distel has some drawbacks which make it hard to apply
it to real-world data.  One of these is the fact that every real-world data set contains
\emph{errors}, and in such cases considering only terminological knowledge that is
\emph{valid} in this erroneous data seems futile.  In the worst case, valid terminological
knowledge is not extracted from the data set because a single error invalidates it.

On the other hand, learning terminological knowledge from an interpretation or linked data
is only reasonable if this data set is of sufficiently high \emph{quality}, meaning that
it does not contain too many errors that are relevant for the learning process.  Then one
could imagine that it may be fruitful to also consider terminological knowledge that is
\emph{almost valid} in this data.  In this way, knowledge that is erroneously invalidated
by few errors is still recovered from the data.  Of course, one has to make sure that rare
but valid counterexamples in this data set are not accidentally ignored in this way, \ie
the knowledge obtained needs to be verified by an external source of information, like a
human expert.

It is the purpose of this work to present an extension of the results by Baader and Distel
that incorporates the idea of considering general concept inclusions which are almost
valid in the input data.  To this end, we shall make use of the notion of
\emph{confidence} from data mining, and transfer it to general concept inclusions.  We
then shall show how we can learn all general concept inclusions with \emph{high
  confidence} in our input data.  For this we use ideas from formal concept analysis, more
specifically results obtained by Michael Luxenburger on \emph{partial implications} in
formal contexts.  We then shall apply our findings to a small example interpretation
extracted from the DBpedia data set, and assess in how far our approach yields more
error-tolerant results.  Finally, we shall show how we can obtain a semi-automatic
algorithm for the expert-based verification step mentioned above, based on the algorithm
of \emph{attribute exploration} from formal concept analysis.

All these extensions are based on the original results by Baader and Distel, and we shall
review these results to the extent needed for our considerations.  This will also include
an introduction to the main notions of formal concept analysis and description logics, as
necessary for this work.  It is the aspiration that this extension of Baader's and
Distel's results make the techniques of extracting general concept inclusions from finite
interpretations more accessible for practical applications.

\bigskip\noindent%
\textit{Acknowledgments}\hspace*{1em} As any work of this size, this thesis would not have
been possible without the (explicit or implicit) help of many people.  First and foremost,
I would like to express my thanks to my supervisors, Prof.\,Bernhard Ganter, Prof.\,Franz
Baader, and Prof.\,Gerd Brewka, for their encouragement and help during the development of
this dissertation, as well as for their financial support.  I am particularly grateful for
the opportunity of being a scholarship holder of the Research Training Group 1763
\enquote{QuantLA} during the last two years, which gave me the necessary time and
confidence to work out what has previously been only a loose collection of vague ideas.

One of the biggest obstacles to finishing this project certainly was to sustain the
motivation for doing so, and I owe special thanks to a lot of people for helping me in
this, some of them maybe without knowing it.  In particular, I am grateful to all members
of the Institute of Theoretical Computer Science, as well as to all members of the
Institute of Algebra, for fruitful discussions ranging from technical topics up to
seemingly unrelated things, that however helped me to forget about this work for a while,
giving me the necessary pause.  This especially includes Felix Distel, whom I had the
honor to work with for nearly four years, and on whose Ph.\,D.\, thesis the present book
is based.  I am also grateful to the many anonymous reviewers who thoroughly pointed out
errors in my papers, and thus helped to improve the quality of my results.  Finally, I
have to thank Prof.\,Ulrike Baumann for sharing with me her wisdom about life, which
helped to overcome more than one crisis.

This book is typeset with \LaTeX, the only professional and free typesetting system I am
aware of that is able to produce typographic results of high (and thus acceptable)
quality.  Using \LaTeX\ together with Emacs on Debian GNU/Linux gave me just the right
tools to master the complexities a book of over 200 pages brings with it, and even allowed
me to have fun while writing it.  Thanks to all who contributed to these tools, and who
contributed to free software in general!

What I am particularly bad at is proof-reading, and thus I am deeply indebted to my two
personal lectors, Juliane Prochaska and Tom Hanika, for carefully reading \emph{the whole
  book} and giving a lot helpful hints.  Without them, many sentences would still be too
long to be understandable.

One of the deepest insights I have gained during the past years is that there is much more
to life than working, and there is no deeper, more profound, and more lasting experience
than receiving this insight from ones own children.  Thank you, Tabea and Fiona, for
showing me the right way.  And thank you, Juliane, for our wonderful children.  Thank you!

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../main"
%%% End: 

%  LocalWords:  Universtiät
